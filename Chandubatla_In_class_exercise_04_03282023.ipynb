{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharunchandubatla/Tharun_INFO5731_Fall2023/blob/main/Chandubatla_In_class_exercise_04_03282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLRcqoqE5_xU"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS_nv7hO5_xY"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBKJBQGT5_xY"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rsEBCi95_xZ",
        "outputId": "c3a1c75f-044d-4dcd-c432-7217e20280c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: (0, '0.111*\"movie\" + 0.111*\"it.\" + 0.111*\"loved\" + 0.111*\"was\" + 0.111*\"The\" + 0.111*\"I\" + 0.111*\"excellent!\" + 0.014*\"is\" + 0.014*\"amazing\" + 0.014*\"with\"')\n",
            "Topic 2: (1, '0.044*\"movie\" + 0.044*\"was\" + 0.043*\"boring.\" + 0.043*\"disappointing.\" + 0.043*\"is\" + 0.043*\"This\" + 0.043*\"An\" + 0.043*\"actors!\" + 0.043*\"amazing\" + 0.043*\"bad\"')\n",
            "Topic 3: (2, '0.044*\"movie\" + 0.044*\"was\" + 0.043*\"An\" + 0.043*\"bad\" + 0.043*\"This\" + 0.043*\"so\" + 0.043*\"is\" + 0.043*\"excellent!\" + 0.043*\"It\" + 0.043*\"boring.\"')\n",
            "Topic 4: (3, '0.111*\"was\" + 0.111*\"a\" + 0.111*\"terrible\" + 0.111*\"film...\" + 0.111*\"It\" + 0.111*\"so\" + 0.111*\"disappointing.\" + 0.014*\"movie\" + 0.014*\"bad\" + 0.014*\"An\"')\n",
            "Topic 5: (4, '0.044*\"movie\" + 0.043*\"was\" + 0.043*\"is\" + 0.043*\"bad\" + 0.043*\"amazing\" + 0.043*\"with\" + 0.043*\"disappointing.\" + 0.043*\"boring.\" + 0.043*\"actors!\" + 0.043*\"This\"')\n",
            "Topic 6: (5, '0.140*\"movie\" + 0.075*\"and\" + 0.075*\"with\" + 0.075*\"great\" + 0.075*\"actors!\" + 0.075*\"This\" + 0.075*\"boring.\" + 0.075*\"An\" + 0.075*\"bad\" + 0.075*\"amazing\"')\n",
            "Topic 7: (6, '0.044*\"movie\" + 0.044*\"was\" + 0.043*\"amazing\" + 0.043*\"disappointing.\" + 0.043*\"bad\" + 0.043*\"is\" + 0.043*\"film...\" + 0.043*\"This\" + 0.043*\"actors!\" + 0.043*\"boring.\"')\n",
            "Topic 1: movie, it., loved, was, The, I, excellent!, is, amazing, with\n",
            "Topic 2: movie, was, boring., disappointing., is, This, An, actors!, amazing, bad\n",
            "Topic 3: movie, was, An, bad, This, so, is, excellent!, It, boring.\n",
            "Topic 4: was, a, terrible, film..., It, so, disappointing., movie, bad, An\n",
            "Topic 5: movie, was, is, bad, amazing, with, disappointing., boring., actors!, This\n",
            "Topic 6: movie, and, with, great, actors!, This, boring., An, bad, amazing\n",
            "Topic 7: movie, was, amazing, disappointing., bad, is, film..., This, actors!, boring.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Define your text documents (corpus)\n",
        "documents = [\n",
        "    \"The movie was excellent! I loved it.\",\n",
        "    \"It was a terrible film... so disappointing.\",\n",
        "    \"An amazing movie with great actors!\",\n",
        "    \"This movie is bad and boring.\"\n",
        "]\n",
        "\n",
        "# Preprocessing the text data\n",
        "# You can tokenize, remove stopwords, and apply other preprocessing steps here\n",
        "\n",
        "# Tokenize the text documents\n",
        "tokenized_documents = [doc.split() for doc in documents]\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(tokenized_documents)\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_documents]\n",
        "\n",
        "# Compute coherence scores for different numbers of topics\n",
        "coherence_scores = []\n",
        "for num_topics in range(2, 11):  # You can adjust the range as needed\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_documents, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "# Find the optimal number of topics with the highest coherence score\n",
        "optimal_num_topics = range(2, 11)[coherence_scores.index(max(coherence_scores))]\n",
        "\n",
        "# Train the final LDA model with the optimal number of topics\n",
        "final_lda_model = LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary)\n",
        "\n",
        "# Print the discovered topics\n",
        "topics = final_lda_model.print_topics()\n",
        "for topic_num, topic_words in enumerate(topics):\n",
        "    print(f\"Topic {topic_num + 1}: {topic_words}\")\n",
        "\n",
        "# Summarize the topics by listing the most representative words in each topic\n",
        "for topic_num, topic_words in enumerate(topics):\n",
        "    top_words = [word for word, _ in final_lda_model.show_topic(topic_num)]\n",
        "    print(f\"Topic {topic_num + 1}: {', '.join(top_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-AfvL25_xa"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBgXXXMg5_xa",
        "outputId": "ac216dc8-47c0-4782-c961-cbca5e25a599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: it, was, movie, the, loved\n",
            "Topic 2: movie, with, amazing, an, great\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Define a list of text documents\n",
        "documents = [\n",
        "    \"The movie was excellent! I loved it.\",\n",
        "    \"It was a terrible film... so disappointing.\",\n",
        "    \"An amazing movie with great actors!\",\n",
        "    \"This movie is bad and boring.\"\n",
        "]\n",
        "\n",
        "# Text Preprocessing\n",
        "# You can tokenize, remove stopwords, and apply other text preprocessing techniques here\n",
        "\n",
        "# Create a TF-IDF matrix to represent the documents\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Choose the number of topics (K) based on your needs\n",
        "num_topics = 2  # You can adjust this number according to your problem\n",
        "\n",
        "# Apply Latent Semantic Analysis (LSA) using TruncatedSVD\n",
        "lsa = TruncatedSVD(n_components=num_topics)\n",
        "lsa_topics = lsa.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Summarize the LSA topics\n",
        "for topic_num, topic in enumerate(lsa.components_):\n",
        "    # Select the top 5 words for each topic\n",
        "    top_words_indices = topic.argsort()[::-1][:5]\n",
        "    top_words = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_words_indices]\n",
        "    print(f\"Topic {topic_num + 1}: {', '.join(top_words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IBbcD7u5_xb"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XGTjOgfM5_xb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iAjLGtO5_xc"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iSjaRPMT5_xc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdGlw9MP5_xd"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modeling methods like LDA, LSA, lda2vec, and BERTopic provide outputs that may be compared depending on the nature of your data and your desired outcomes. Each algorithm has its own advantages and disadvantages, hence there is no universal solution. In the following, I'll detail the most important considerations for making a decision between these algorithms and provide some suggestions as to which could work best in certain situations.\n",
        "\n",
        "Interpretability:\n",
        "\n",
        "LDA: LDA is noted for its great interpretability. It generates subjects in the form of a word list, making it simple to recognize and categorize content. This is useful when you require human-readable and interpretable subjects.\n",
        "\n",
        "Similarly to LDA, LSA material is interpretable, albeit maybe less so than LDA. Singular value decomposition-based subjects may not be as easily readable to humans as LDA ones.\n",
        "\n",
        "Although lda2vec shares LDA and LSA's goal of capturing semantic meaning in topics, it may not provide the same degree of topic interpretability. Its primary goal is to understand the text's implied meaning.\n",
        "\n",
        "BERTopic: BERTopic employs contextual embeddings, which may provide interpretably rich topics. However, it may be resource-intensive to compute, particularly for big datasets.\n",
        "\n",
        "In this scenario, LDA is a great option since it produces highly interpretable topics. BERTopic may be useful if you need to strike a balance between interpretability and capturing semantic meaning.\n",
        "\n",
        "Effectiveness and scalability:\n",
        "\n",
        "In most cases, LDA will be both effective and scalable when dealing with text data. It has several uses and is thus quite popular.\n",
        "\n",
        "LSA: LSA works quickly and effectively, even with massive text databases.\n",
        "\n",
        "For bigger datasets, lda2vec may be resource-intensive because to its high processing cost.\n",
        "\n",
        "Due to its usage of BERT embeddings, BERTopic is more computationally costly and may not be appropriate for extremely big datasets.\n",
        "\n",
        "The efficiency of LDA and LSA makes them preferable for use with huge amounts of text data in many applications. Smaller or somewhat big datasets may fare better when using lda2vec or BERTopic.\n",
        "\n",
        "Excellent Content:\n",
        "\n",
        "When compared to other approaches, LDA's less-than-syntactically-meaningful subjects may result from its reliance on word co-occurrence patterns.\n",
        "\n",
        "High-quality themes may be provided using latent semantic analysis (LSA) topics. However, they aren't always as open to interpretation.\n",
        "\n",
        "The goal of lda2vec is to improve the quality of subjects by capturing more of their semantic content.\n",
        "\n",
        "BERTopic: BERTopic employs cutting-edge BERT embeddings, which may provide remarkably cohesive and informative topics.\n",
        "\n",
        "In this scenario, high-quality topics may be obtained with BERTopic and lda2vec.\n",
        "\n",
        "Customization:\n",
        "\n",
        "LDA: LDA provides for some degree of customisation by adjusting parameters such the number of topics, alpha, and beta.\n",
        "\n",
        "When contrasted to LDA, LSA allows for less individualized adjustment.\n",
        "\n",
        "lda2vec: lda2vec provides for customisation of hyperparameters, but its complexity may demand a detailed grasp of its parameters.\n",
        "\n",
        "While BERTopic does allow for some hyperparameter modification, it may not be as versatile as LDA when it comes to making adjustments.\n",
        "\n",
        "Use Case: LDA may be preferable since it allows for granular adjustment of parameters, making it a good option when a high level of customisation is required.\n",
        "\n",
        "In conclusion, the \"best\" subject modeling approach relies on your individual aims and dataset features. LDA and LSA are excellent choices if you place a premium on interpretability. Both BERTopic and lda2vec may be useful if you need high-quality topics but can't avoid a certain amount of processing cost. Considerations such as effectiveness, scalability, and adaptability are highly recommended. You should weigh the pros and cons of each algorithm and choose the one that works best for your needs. Trying out several algorithms until you discover the one that works best for your application is a common best practice."
      ],
      "metadata": {
        "id": "tn0HpcReHZ1p"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}