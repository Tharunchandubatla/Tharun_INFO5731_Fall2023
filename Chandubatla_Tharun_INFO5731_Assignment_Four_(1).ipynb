{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharunchandubatla/Tharun_INFO5731_Fall2023/blob/main/Chandubatla_Tharun_INFO5731_Assignment_Four_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98ea4d6-6852-4529-d2f8-6a71852d5876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "['original', 'movie', 'mission', 'impossible', 'great', 'perfect', 'gun', 'like', 'tom', 'cruise']\n",
            "\n",
            "Topic #2:\n",
            "['just', 'movie', 'film', 'great', 'story', 'especially', 'tom', 'cruise', 'going', 'plot']\n",
            "\n",
            "Topic #3:\n",
            "['film', 'movie', 'gun', 'maverick', 'great', 'just', 'story', 'feel', 'ride', 'like']\n",
            "\n",
            "Topic #4:\n",
            "['action', 'gun', 'film', 'good', 'cruise', 'watch', 'tom', 'felt', 'maverick', 'big']\n",
            "\n",
            "Topic #5:\n",
            "['movie', 'scene', 'original', 'flying', 'great', 'film', 'years', 'cast', 'kilmer', 'theater']\n",
            "\n",
            "Topic #6:\n",
            "['new', 'film', 'maverick', 'original', 'music', 'love', 'great', 'actually', 'cruise', 'adding']\n",
            "\n",
            "Topic #7:\n",
            "['cheese', 'movie', 'like', 'maverick', 'tom', 'flying', 'cruise', 'scenes', 'years', 'teller']\n",
            "\n",
            "Topic #8:\n",
            "['maverick', 'cruise', 'action', 'film', 'original', 'gun', 'new', 'big', 'great', 'pilots']\n",
            "\n",
            "Topic #9:\n",
            "['movie', 'action', 'gun', 'scenes', 'best', 'saw', 'time', 'wasn', 'maverick', 'perfect']\n",
            "\n",
            "Topic #10:\n",
            "['movie', 'story', 'just', 'big', 'need', 'real', 'know', 'age', 'time', 'especially']\n",
            "\n",
            "Top 10 Clusters:\n",
            "6    1600\n",
            "1    1600\n",
            "8    1200\n",
            "3    1200\n",
            "4    1200\n",
            "7     800\n",
            "5     800\n",
            "2     800\n",
            "0     400\n",
            "9     400\n",
            "Name: topic, dtype: int64\n",
            "\n",
            "Cluster #7 Summary:\n",
            "Example Text: I was reluctantly dragged into the theater, thinking that they didn't need to make a Top Gun 2 and that the first one was where that story needed to end.I could write a couple paragraphs to summarize my feelings after walking out of the theater, but I'm going to leave it with just one sentence.I was wrong.\n",
            "Number of Documents in Cluster: 1600\n",
            "\n",
            "\n",
            "Cluster #2 Summary:\n",
            "Example Text: I don't share everyone's unbridled enthusiasm for this film. It is indeed a great popcorn flick, with outstanding aerial photography and maneuvers. But 10 stars? There are few, if any, movies that are perfect, and deserve that kind of rating.The problem with the film is the plot. It is so filled with age-worn cliches that one could easily tell what was coming from beginning to end. I mean, you had to know who was going to save the day at the end, and you had to know what was going to happen when Maverick jumped out of Penny's window. Those are just two examples of the many obvious plot points that you could see coming a mile away. I could list them all, but it would take up too much space here. Basically the entire plot was entirely predictable.The opening scene, especially, was straight out of Hollywood Screenplay Writing 101. I mean, seriously, how many times have we seen that subplot? Countless.There were no characters in the movie, either. They were all caricatures, stereotypes. No depth to any of them. They had their standard roles to play, and that was it.Did I enjoy the film? Sure, it was fun. Especially on a big theater screen with a loud sound system. Did I take anything away from the film? Did it make me think about anything after it was over? Nah. Will I see it again? Nah.I will give Tom Cruise credit for including Val Kilmer in the cast. Considering his health problems, that was a nice touch.So, yeah, enjoy the film. Sit back with your bag of popcorn and enjoy the g-forces. But don't pretend it is anything other than just another summer blockbuster.\n",
            "Number of Documents in Cluster: 1600\n",
            "\n",
            "\n",
            "Cluster #9 Summary:\n",
            "Example Text: If there's any movie that deserves to be seen in the theaters with big screens and booming speakers. It's :Top Gun Maverick.One of my best experiences in years!\n",
            "Number of Documents in Cluster: 1200\n",
            "\n",
            "\n",
            "Cluster #4 Summary:\n",
            "Example Text: There are so many references to the original that I was brought to childhood when I had first seen it.The best cinematic experience of my life and a perfect gift to all Top Gun fans.\n",
            "Number of Documents in Cluster: 1200\n",
            "\n",
            "\n",
            "Cluster #5 Summary:\n",
            "Example Text: Wow! As a retired combat veteran who has flown jets, I LOVED THIS MOVIE! It had massive heart, great action and wonderful Cameos. The flying scenes were the best I've seen. Nobody left the theater for 90 minutes!\n",
            "Number of Documents in Cluster: 1200\n",
            "\n",
            "\n",
            "Cluster #8 Summary:\n",
            "Example Text: In one of the more memorable lines in the original Top Gun, Maverick gets chewed out by a superior who tells him, \"Son, your ego's writing checks your body can't cash.\" Sometimes I wonder if Tom Cruise took that putdown as a personal challenge. No movie star seems to work harder or push himself further than Cruise these days. Ridiculously entertaining Top Gun: Maverick. Cruise was in his early 20s when he first played Pete \"Maverick\" Mitchell, the cocky young Navy pilot with the aviator sunglasses, the Kawasaki motorcycle and the need for speed. In the sequel, he's as arrogant and insubordinate as ever: Now a Navy test pilot in his late 50s, Maverick still knows how to tick off his superiors, as we see in an exciting opening sequence where he pushes a new plane beyond its limits. Partly as punishment, he's ordered to return to TOPGUN, the elite pilot-training school, and train its best and brightest for an impossibly dangerous new mission. And so the three screenwriters of Top Gun: Maverick - including Cruise's regular Mission: Impossible writer-director, Christopher McQuarrie - have taken the threads of the original and spun them into an intergenerational male weepie, a dad movie of truly epic proportions. They're tapping into nostalgia for the original, while aiming for new levels of emotional grandeur. To that end, the soundtrack features a Lady Gaga song, \"Hold My Hand.\" It's nowhere near as iconic a chart topper as the original movie's \"Take My Breath Away,\" but tugs at your heartstrings nonetheless. The action sequences are much more thrilling and immersive than in the original. You feel like you're really in the cockpit with these pilots, and that's because you are: The actors underwent intense flight training and flew actual planes during shooting. In that respect, Top Gun: Maverick feels like a throwback to a lost era of practical moviemaking, before computer-generated visual effects took over Hollywood. You start to understand why Cruise, the creative force behind the movie, was so driven to make it: In telling a story where older and younger pilots butt heads, and state-of-the-art F-18s duke it out with rusty old F-14s, he's trying to show us that there's room for the old and the new to coexist. He's also advancing a case for the enduring appeal of the movies and their power to transport us with viscerally gripping action and big, sweeping emotions.\n",
            "Number of Documents in Cluster: 800\n",
            "\n",
            "\n",
            "Cluster #6 Summary:\n",
            "Example Text: What an excellent sequel - I, in fact, like it more than its predecessor.'Top Gun: Maverick' is fantastic, simply put. I was expecting it to be good, but it's actually much more enjoyable than I had anticipated. The callbacks to the original are expertly done, the new characters are strong/well cast, it has plenty of meaning, music is fab and the action is outstanding - the aerial stuff is sensational.The story is superb, with each high stake coming across as intended - parts even gave me slight goosebumps, which is a surprise given I'm not someone who has a connection to the 1986 film. It's all super neatly put together, I honestly came close to giving it a higher rating.Tom Cruise is brilliant as he reprises the role of Maverick, while Miles Teller comes in and gives a top performance. Jennifer Connelly is another positive, though her role does kinda feel a tiny bit forced in order to have a love interest; given Kelly McGillis' (unexplained) absence.Monica Barbaro stands out most from the fresh faces, though I actually did enjoy watching them all - which is something I thought the film may struggle with, adding new people, but it's done nicely; sure Jon Hamm and Glen Powell are a little cliché, though overall I approve.A great watch - I'd highly recommend it, though naturally would suggest watching the previous film first if you haven't already.\n",
            "Number of Documents in Cluster: 800\n",
            "\n",
            "\n",
            "Cluster #3 Summary:\n",
            "Example Text: Based on the scores, I was expecting a much better film. But I now get it., its the nostalgia for the first film that fuels the scores for this film.Here is my take and why I think its an \"okay\" film, just not a great one. There really isn't a whole lot of meat and potatoes to this story. It's largely a copy pasta in large chunks from the older film but with newer,\n",
            "younger faces. Maverick is charged with training a squadron of top gun fighter pilots to destroy an enemy uranium enrichment plant that is about to come online and operational. But it seems the producers want to maximize the profits for this film and not offend anyone in the real world. So this enemy doesn't have a name or a geographical location. The enemy has 5th generation fighter jets and some James Bond evil villian sized SAM batteries around the mountain's summit. It is a snow covered terrain so that rules out many enemies in warmer climates.Then there is the lone aircraft carrier delivering these jet fighters to the enemies doorstep. Well, by now most people know that wouldn't be the case. Carriers are heavily guarded and their presence would trigger the attention of any adversary. No way could four F18 sneak in under radar to blow up this plant unless we accept the enemy isn't aware of a huge freakin carrier of their shores. So there is nothing resembling a realistic scenario and that kept me from enjoying the story too much.The romantic relationship in the film has no chemistry. I just didn't feel any chemistry between Penny and Maverick. That's a first because you do have two very good looking actors and I always thought Tom Cruise to be great action actor. So no chemistry between Cruise and Jennifer Connelly is a bitter pill to swallow.What I did like about it was the cinematography and some of the action sequences with the dog fighting. But I didn't think the dog fighting were any better than the original film. In fact, the original film might have had better scenes. This film just has a lot of choppy editing to imply action.I think its a 6/10, but not a chance it's a 9/10. Maybe if they added a Tom Clancy inspired screenplay to the film I might have given it a 9. Maybe a team of seals to do recon before having the jets engage and with the aircraft carrier companied by a battlegroup flotilla. Aircraft Carriers just don't end up secretly off the shore of adversarial country completely alone, not in the real world.\n",
            "Number of Documents in Cluster: 800\n",
            "\n",
            "\n",
            "Cluster #1 Summary:\n",
            "Example Text: See? This is why I LOVE Tom Cruise Post-Mid-1992. Aside from the RARE misfire, cough, cough, The Mummy, when Tom Cruise wants a movie to be better than perfect, he pushes it past Mach 10.The first half of this adventure made me feel this is the perfect and worthy sequel to the original. AND THEN...the second half kicked in thoroughly SURPASSING the original by a landslide. Now, it certainly will not have the impact on the 2020s as the original did for the 1980s - that's lightning that can't be caught twice. But, rest assured: this simultaneously saluted the 1986 classic and then told it to hold its beer.Maverick never changed. And just went he was about to use up his seemingly endless supply of luck, he got recruited back into the \"Top Gun\" division to get the elite ready for a, dare I?, almost Impossible Mission. Oh, and there's a ton of drama and obligatory dick-measuring thrown in.What matters most is: the setup for the film takes place perfectly in the first act giving us clear direction (something the original lacked,) and gave us goals with character building to anticipate and cheer on. When the climax began, it's WAS exactly what I wanted, until it went on another 15 minutes to make the movie SOAR beyond my expectations.While the acting was superb from the Veterans, it wasn't all that great from the fresh new recruits. That was to be expected. Much like their characters in the movie, they're learning to be the best. And they may someday.The movie's music is not as great as the original, but the movie used it sparingly, correctly and with some great new tracks. The original knew it had a killer soundtrack and overdid it. This one showed the restraint it needed.If you loved the first one, there's virtually no way you wouldn't love this even more. It's absolutely perfect for those who were there buying tickets for the original and it's also great for newbies so they can learn to appreciate the classics and understand: with age comes experience and that should never be mocked. It should be learned from and used to progress.And to Top it all off, all of the practical stunts and aerial shots made 10x more sense and easier to follow this time around. I just rewatched the original last night for only the second time (the first being on home video in 2015) and my head was literally spinning to follow the action in the sky. Here, they make sure no one is left in the dust.I can't recommend this movie enough and to see it in theatres, preferably a Dolby-certified one. Don't wait for Paramount+ to pick this up (like I normally would.) Go see this now on the big screen as it should be seen and don't miss out like I did with the original in 1986.***Final Thoughts: I *just* made my Top Ten Tom Cruise movie list, time to update it:1. A Few Good Men\n",
            "2. Mission: Impossible - Fallout\n",
            "3. Edge of Tomorrow\n",
            "4. Mission: Impossible - Rogue Nation\n",
            "5. Mission: Impossible - Ghost Protocol\n",
            "6. Top Gun: Maverick\n",
            "7. Collateral\n",
            "8. Jerry Maguire\n",
            "9. Knight and Day\n",
            "10. Mission: Impossible IIIHonorable Mentions:11. Top Gun\n",
            "12. Oblivion\n",
            "13. The Firm\n",
            "14. Interview with the Vampire\n",
            "15. Rain Man\n",
            "16. Vanilla Sky.\n",
            "Number of Documents in Cluster: 400\n",
            "\n",
            "\n",
            "Cluster #10 Summary:\n",
            "Example Text: It makes more than sense to have seen the first Maverick movie. I am writing this with that in mind. Actually I would even argue that the rating also had something to do with it. Watching it on the big screen also helps a lot of course. My summary line is actually a quote from the movie. Not just the first one, but one that is being said here too. And you will get the full impact of that sentence, of those words, if you know what they mean and why they are said.There are quite a few connections to the first movie, but there is also nods to real life. Especially with Val Kilmer in mind. And all is serving the story - even the real life struggle Kilmer has, that gets woven into the character Iceman. So there are two of the big stars of the first one name checked - what about Kelly McGillis though? Well don't hold your breath (it is being taken away, sorry for the pun)! She is not returning. Now you could be mad at the movie for that - or you could salute it for bringing \"back\" a character that was only name checked in the first movie - and being played by the wonderful Jennifer Connely.Now is she as beautiful as ever? Yes. But she is also close to the age of Tom Cruise that some other names could have been. Of course do not overthink the age thing, because you'd find that her real age would have been inappropriate during the time of the first movie ... just saying. Again: do not overthink it at all.Those things out of the way, we get introduced to a whole bunch of new characters. There probably is not as much ... well male on male energy here as there was in the first one. Even in the dialog - you just feel it. Don't give me that look (and yes I know it is the only one you've got - especially when I bring the puns)! There is more than a coherent story - and the movie takes its time to tell it. There is plenty of action too.There is also technical talk that I didn't totally understand. But it is not necessary to get the movie or enjoy it. And then there is the ending - and what a perfect ending this has. So much so, that I decided it deserves the 10 rating I finally gave it. It may copy many things from the first movie (call it homage), especially the intro but also some story beats, but it still stands ... or rather flies on its own! Just buckle your seatbelt, because you may get dizzy.First time watching I could spot the music cues from the first one, but there were also some fresh elements to it. If I didn't know Lady Gaga was at least partly (big parts if I understood correctly) responsible for the soundtrack, I wouldn't have guessed myself to be honest. As it is, one of the biggest highlights in cinemas this year - the waiting was worth it. If you feel the need .. the need for speed - your need will be satisfied here (or rather in cinemas). Even being able to predict story points will not take away anything from the experience you can have with this. Enjoy.\n",
            "Number of Documents in Cluster: 400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/annotated_dataset.csv')\n",
        "\n",
        "# Replace 'text' with the actual column name containing text data in your dataset\n",
        "text_column = 'User Reviews'\n",
        "\n",
        "# Feature extraction using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_df=0.85, max_features=5000, stop_words='english')\n",
        "X = vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "# Topic modeling using LDA\n",
        "num_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Display top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "    print([feature_names[i] for i in topic.argsort()[:-11:-1]])\n",
        "    print()\n",
        "\n",
        "# Assign topics to documents\n",
        "df['topic'] = lda.transform(X).argmax(axis=1)\n",
        "\n",
        "# Display top 10 clusters for topic modeling\n",
        "top_clusters = df['topic'].value_counts().head(10)\n",
        "print(\"Top 10 Clusters:\")\n",
        "print(top_clusters)\n",
        "\n",
        "# Summarize and describe the topic for each cluster\n",
        "for cluster, count in top_clusters.items():\n",
        "    cluster_df = df[df['topic'] == cluster]\n",
        "    example_text = cluster_df[text_column].iloc[0]  # Take an example document from the cluster\n",
        "    print(f\"\\nCluster #{cluster + 1} Summary:\")\n",
        "    print(f\"Example Text: {example_text}\")\n",
        "    print(f\"Number of Documents in Cluster: {count}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51b1c81-4373-47b1-f834-538cbbce5a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
            "Mean Accuracy: 1.00\n",
            "\n",
            "Logistic Regression Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
            "Mean Accuracy: 1.00\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Naive Bayes:\n",
            "Accuracy: 1.00, Precision: 1.00, Recall: 1.00, F1 Score: 1.00\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy: 1.00, Precision: 1.00, Recall: 1.00, F1 Score: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/annotated_dataset.csv')\n",
        "\n",
        "# Assuming you have a 'text' column for the text data and a 'label' column for sentiment labels\n",
        "# Replace 'text' and 'label' with your actual column names\n",
        "X = df['User Reviews']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Features: TF-IDF representation of the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Model 1: Naive Bayes\n",
        "nb_model = make_pipeline(TfidfVectorizer(max_features=5000, stop_words='english'), MultinomialNB())\n",
        "nb_scores = cross_val_score(nb_model, X, y, cv=5)  # 5-fold cross-validation\n",
        "\n",
        "# Model 2: Logistic Regression\n",
        "lr_model = make_pipeline(TfidfVectorizer(max_features=5000, stop_words='english'), LogisticRegression())\n",
        "lr_scores = cross_val_score(lr_model, X, y, cv=5)  # 5-fold cross-validation\n",
        "\n",
        "# Display results\n",
        "print(\"Naive Bayes Cross-Validation Scores:\", nb_scores)\n",
        "print(\"Mean Accuracy: {:.2f}\".format(nb_scores.mean()))\n",
        "\n",
        "print(\"\\nLogistic Regression Cross-Validation Scores:\", lr_scores)\n",
        "print(\"Mean Accuracy: {:.2f}\".format(lr_scores.mean()))\n",
        "\n",
        "# Train and evaluate models on the test set\n",
        "nb_model.fit(X_train, y_train)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nEvaluation on Test Set:\")\n",
        "print(\"\\nNaive Bayes:\")\n",
        "nb_accuracy, nb_precision, nb_recall, nb_f1 = evaluate_model(nb_model, X_test, y_test)\n",
        "print(\"Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\".format(nb_accuracy, nb_precision, nb_recall, nb_f1))\n",
        "\n",
        "print(\"\\nLogistic Regression:\")\n",
        "lr_accuracy, lr_precision, lr_recall, lr_f1 = evaluate_model(lr_model, X_test, y_test)\n",
        "print(\"Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\".format(lr_accuracy, lr_precision, lr_recall, lr_f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54efdb61-fffd-4c09-d9ce-a37627961ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 4262060818.240992\n",
            "R-squared: 0.44434425562664803\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load training data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Assuming 'SalePrice' is the target variable\n",
        "y = train_data['SalePrice']\n",
        "X = train_data.drop('SalePrice', axis=1)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), X.columns.difference(categorical_cols)),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a linear regression model with preprocessing\n",
        "model = make_pipeline(preprocessor, SimpleImputer(strategy='mean'), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n",
        "\n",
        "# Load testing data\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Assuming the testing data has the same features as the training data\n",
        "# If not, you need to preprocess the testing data accordingly\n",
        "\n",
        "# Make predictions on the testing set\n",
        "test_predictions = model.predict(test_data)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "submission_df = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_predictions})\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **pre-trained Large Language Model (LLM) from the Hugging Face Repository** for your specific task using the data collected in Assignment 3. After creating an account on Hugging Face (https://huggingface.co/), choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any Meta based text analysis model. Provide a brief description of the selected LLM, including its original sources, significant parameters, and any task-specific fine-tuning if applied.\n",
        "\n",
        "Perform a detailed analysis of the LLM's performance on your task, including key metrics, strengths, and limitations. Additionally, discuss any challenges encountered during the implementation and potential strategies for improvement. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with your actual dataset)\n",
        "df = pd.read_csv('/content/annotated_dataset.csv')\n",
        "\n",
        "# Assuming you have 'User Reviews' and 'sentiment' columns\n",
        "X = df['User Reviews'].values\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values  # Convert 'positive' to 1, 'negative' to 0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use a subset for development\n",
        "X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 for binary classification\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt', max_length=256)\n",
        "X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt', max_length=256)\n",
        "\n",
        "# Create DataLoader for training and testing sets\n",
        "train_dataset = TensorDataset(X_train_tokens['input_ids'], X_train_tokens['attention_mask'], torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(X_test_tokens['input_ids'], X_test_tokens['attention_mask'], torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Fine-tune the BERT model on your task (training loop)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1  # Reduce the number of epochs for testing\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
        "        predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Calculate key metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20vl8e41leYh",
        "outputId": "5e0cdcb9-c08c-4830-d212-cc9c9376fb77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}