{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharunchandubatla/Tharun_INFO5731_Fall2023/blob/main/Chandubatla_Tharun_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3174472-003d-4cf5-8204-00e1b60f749a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 10000 user reviews and saved to Top Gun: Maverick_user_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the IMDb page for the movie \"Top Gun: Maverick\"\n",
        "url = \"https://www.imdb.com/title/tt1745960/reviews\"\n",
        "\n",
        "# Function to scrape user reviews\n",
        "def scrape_imdb_reviews(url, num_reviews=10000):\n",
        "    reviews = []\n",
        "    page = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        page_url = f\"{url}?sort=helpfulnessScore&dir=desc&ratingFilter=0&spoiler=hide&ref_=tt_ov_rt&page={page}\"\n",
        "        response = requests.get(page_url)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        user_reviews = soup.find_all(\"div\", class_=\"text show-more__control\")\n",
        "\n",
        "        for review in user_reviews:\n",
        "            text = review.get_text(strip=True)\n",
        "            reviews.append(text)\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "# Movie name\n",
        "movie_name = \"Top Gun: Maverick\"\n",
        "\n",
        "# Scrape reviews\n",
        "reviews = scrape_imdb_reviews(url)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(reviews, columns=[\"User Reviews\"])\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_file_name = f\"{movie_name}_user_reviews.csv\"\n",
        "data.to_csv(csv_file_name, index=False)\n",
        "print(f\"Collected {len(reviews)} user reviews and saved to {csv_file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046cc50c-3052-4598-96c0-69c138f45ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to Top_Gun_Maverick_cleaned_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (stopwords and lemmatization data)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file with user reviews\n",
        "csv_file_name = \"Top Gun: Maverick_user_reviews.csv\"\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Define functions for text cleaning\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuations\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "\n",
        "    # Remove numbers\n",
        "    text = ''.join([char for char in text if not char.isdigit()])\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def stem_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    words = text.split()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the cleaning functions to the \"User Reviews\" column\n",
        "df['Cleaned Reviews'] = df['User Reviews'].apply(clean_text)\n",
        "df['Cleaned Reviews'] = df['Cleaned Reviews'].apply(remove_stopwords)\n",
        "df['Cleaned Reviews'] = df['Cleaned Reviews'].apply(stem_text)\n",
        "df['Cleaned Reviews'] = df['Cleaned Reviews'].apply(lemmatize_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "cleaned_csv_file_name = \"Top_Gun_Maverick_cleaned_reviews.csv\"\n",
        "df.to_csv(cleaned_csv_file_name, index=False)\n",
        "print(f\"Cleaned data saved to {cleaned_csv_file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load the cleaned data from the CSV file\n",
        "cleaned_csv_file_name = \"Top_Gun_Maverick_cleaned_reviews.csv\"\n",
        "df = pd.read_csv(cleaned_csv_file_name)\n",
        "\n",
        "# Initialize spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging and count POS categories\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = {\"Noun\": 0, \"Verb\": 0, \"Adjective\": 0, \"Adverb\": 0}\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            pos_counts[\"Noun\"] += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            pos_counts[\"Verb\"] += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            pos_counts[\"Adjective\"] += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            pos_counts[\"Adverb\"] += 1\n",
        "\n",
        "    return pos_counts\n",
        "\n",
        "# Function to perform constituency parsing\n",
        "def constituency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    constituency_tree = \"\"\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            constituency_tree += f\"({token.text} ({token.dep_} \"\n",
        "        constituency_tree += \")\"\n",
        "\n",
        "    return constituency_tree\n",
        "\n",
        "# Function to perform dependency parsing\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            print(token.text, token.dep_, token.head.text)\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER)\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entity_type = ent.label_\n",
        "        if entity_type in entities:\n",
        "            entities[entity_type] += 1\n",
        "        else:\n",
        "            entities[entity_type] = 1\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Example sentence for explanation\n",
        "example_sentence = df['Cleaned Reviews'][0]\n",
        "\n",
        "# Perform POS tagging and count POS categories\n",
        "pos_counts = pos_tagging(example_sentence)\n",
        "print(\"POS Tagging:\", pos_counts)\n",
        "\n",
        "# Perform constituency parsing\n",
        "constituency_tree = constituency_parsing(example_sentence)\n",
        "print(\"Constituency Parsing Tree:\", constituency_tree)\n",
        "\n",
        "# Perform dependency parsing\n",
        "print(\"Dependency Parsing:\")\n",
        "dependency_parsing(example_sentence)\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "entities = named_entity_recognition(example_sentence)\n",
        "print(\"Named Entity Recognition:\", entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL8tQmJLoneq",
        "outputId": "f2453ba0-82de-478f-bbad-a017e35d2468"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging: {'Noun': 91, 'Verb': 33, 'Adjective': 25, 'Adverb': 9}\n",
            "Constituency Parsing Tree: (one (nummod (memor (compound (line (compound (origin (nmod (top (amod (gun (compound (maverick (nsubj (get (ROOT (chew (nsubj (superior (amod (tell (ccomp (son (compound (ego (compound (write (compound (check (compound (bodi (dobj (ca (aux (nt (neg (cash (advcl (sometim (compound (wonder (dobj (tom (compound (cruis (nsubj (took (conj (putdown (amod (person (compound (challeng (compound (movi (compound (star (nsubj (seem (ccomp (work (nsubj (harder (advmod (push (xcomp (cruis (amod (day (npadvmod (ridicul (compound (entertain (nmod (top (compound (gun (compound (maverick (compound (cruis (compound (earli (nsubj (first (advmod (play (conj (pete (amod (maverick (compound (mitchel (compound (cocki (compound (young (compound (navi (compound (pilot (compound (aviat (compound (sunglass (compound (kawasaki (compound (motorcycl (nsubj (need (ccomp (speed (compound (sequel (dobj (he (nsubj (arrog (relcl (insubordin (conj (ever (amod (navi (compound (test (compound (pilot (dobj (late (amod (maverick (nsubj (still (advmod (know (conj (tick (compound (superior (nsubj (see (compound (excit (nmod (open (amod (sequenc (nsubj (push (ccomp (new (amod (plane (dobj (beyond (prep (limit (compound (partli (pobj (punish (ccomp (he (nsubj (order (compound (return (compound (topgun (nsubj (elit (ccomp (pilottrain (compound (school (compound (train (nmod (best (advmod (brightest (amod (imposs (amod (danger (nmod (new (amod (mission (nmod (three (nummod (screenwrit (compound (top (compound (gun (compound (maverick (compound (includ (nmod (cruis (nmod (regular (amod (mission (compound (imposs (compound (writerdirector (compound (christoph (compound (mcquarri (nsubj (taken (ccomp (thread (compound (origin (dobj (spun (compound (intergener (compound (male (compound (weepi (compound (dad (compound (movi (compound (truli (nmod (epic (compound (proport (compound (theyr (compound (tap (compound (nostalgia (compound (origin (nsubj (aim (conj (new (amod (level (compound (emot (nsubj (grandeur (compound (end (compound (soundtrack (compound (featur (compound (ladi (compound (gaga (compound (song (compound (hold (compound (hand (compound (nowher (nsubj (near (prep (icon (compound (chart (compound (topper (compound (origin (compound (movi (pobj (take (ccomp (breath (dobj (away (advmod (tug (compound (heartstr (dobj (nonetheless (advmod (action (nsubj (sequenc (ccomp (much (amod (thrill (compound (immers (compound (origin (nsubj (feel (ccomp (like (prep (your (poss (realli (compound (cockpit (compound (pilot (pobj (that (mark (actor (compound (underw (compound (intens (compound (flight (compound (train (nsubj (flew (ccomp (actual (amod (plane (compound (shoot (compound (respect (ccomp (top (amod (gun (compound (maverick (dobj (feel (conj (like (mark (throwback (advmod (lost (amod (era (npadvmod (practic (amod (moviemak (compound (computergener (nmod (visual (amod (effect (nsubj (took (advcl (hollywood (compound (start (nsubj (understand (ccomp (cruis (amod (creativ (compound (forc (dobj (behind (prep (movi (pobj (driven (acl (make (advcl (tell (ccomp (stori (dobj (older (amod (younger (amod (pilot (compound (butt (compound (head (nsubj (stateoftheart (ccomp (f (compound (duke (dobj (rusti (compound (old (ccomp )(f (dep (he (nsubj (tri (nsubj (show (ROOT (u (dobj (there (advmod (room (npadvmod (old (amod (new (amod (coexist (dobj (he (nsubj (also (advmod (advanc (ccomp (case (compound (endur (amod (appeal (compound (movi (compound (power (compound (transport (compound (u (compound (viscer (compound (grip (nmod (action (nmod (big (amod (sweep (compound (emot (dobj )\n",
            "Dependency Parsing:\n",
            "one nummod origin\n",
            "memor compound line\n",
            "line compound origin\n",
            "origin nmod maverick\n",
            "top amod maverick\n",
            "gun compound maverick\n",
            "maverick nsubj get\n",
            "get ROOT get\n",
            "chew nsubj tell\n",
            "superior amod chew\n",
            "tell ccomp get\n",
            "son compound bodi\n",
            "ego compound check\n",
            "write compound check\n",
            "check compound bodi\n",
            "bodi dobj tell\n",
            "ca aux cash\n",
            "nt neg cash\n",
            "cash advcl get\n",
            "sometim compound wonder\n",
            "wonder dobj cash\n",
            "tom compound cruis\n",
            "cruis nsubj took\n",
            "took conj get\n",
            "putdown amod person\n",
            "person compound star\n",
            "challeng compound star\n",
            "movi compound star\n",
            "star nsubj seem\n",
            "seem ccomp took\n",
            "work nsubj push\n",
            "harder advmod push\n",
            "push xcomp seem\n",
            "cruis amod day\n",
            "day npadvmod push\n",
            "ridicul compound cruis\n",
            "entertain nmod cruis\n",
            "top compound cruis\n",
            "gun compound cruis\n",
            "maverick compound cruis\n",
            "cruis compound earli\n",
            "earli nsubj play\n",
            "first advmod play\n",
            "play conj seem\n",
            "pete amod maverick\n",
            "maverick compound aviat\n",
            "mitchel compound cocki\n",
            "cocki compound aviat\n",
            "young compound pilot\n",
            "navi compound pilot\n",
            "pilot compound aviat\n",
            "aviat compound motorcycl\n",
            "sunglass compound motorcycl\n",
            "kawasaki compound motorcycl\n",
            "motorcycl nsubj need\n",
            "need ccomp play\n",
            "speed compound sequel\n",
            "sequel dobj need\n",
            "he nsubj arrog\n",
            "arrog relcl sequel\n",
            "insubordin conj seem\n",
            "ever amod pilot\n",
            "navi compound pilot\n",
            "test compound pilot\n",
            "pilot dobj insubordin\n",
            "late amod maverick\n",
            "maverick nsubj know\n",
            "still advmod know\n",
            "know conj get\n",
            "tick compound superior\n",
            "superior nsubj see\n",
            "see compound punish\n",
            "excit nmod sequenc\n",
            "open amod sequenc\n",
            "sequenc nsubj push\n",
            "push ccomp see\n",
            "new amod plane\n",
            "plane dobj push\n",
            "beyond prep push\n",
            "limit compound partli\n",
            "partli pobj beyond\n",
            "punish ccomp know\n",
            "he nsubj return\n",
            "order compound return\n",
            "return compound topgun\n",
            "topgun nsubj elit\n",
            "elit ccomp know\n",
            "pilottrain compound school\n",
            "school compound train\n",
            "train nmod mcquarri\n",
            "best advmod brightest\n",
            "brightest amod danger\n",
            "imposs amod danger\n",
            "danger nmod mission\n",
            "new amod mission\n",
            "mission nmod mcquarri\n",
            "three nummod screenwrit\n",
            "screenwrit compound top\n",
            "top compound mcquarri\n",
            "gun compound maverick\n",
            "maverick compound mcquarri\n",
            "includ nmod mcquarri\n",
            "cruis nmod mcquarri\n",
            "regular amod mcquarri\n",
            "mission compound imposs\n",
            "imposs compound writerdirector\n",
            "writerdirector compound mcquarri\n",
            "christoph compound mcquarri\n",
            "mcquarri nsubj taken\n",
            "taken ccomp get\n",
            "thread compound origin\n",
            "origin dobj taken\n",
            "spun compound intergener\n",
            "intergener compound movi\n",
            "male compound movi\n",
            "weepi compound dad\n",
            "dad compound movi\n",
            "movi compound origin\n",
            "truli nmod origin\n",
            "epic compound proport\n",
            "proport compound theyr\n",
            "theyr compound origin\n",
            "tap compound nostalgia\n",
            "nostalgia compound origin\n",
            "origin nsubj aim\n",
            "aim conj get\n",
            "new amod level\n",
            "level compound emot\n",
            "emot nsubj end\n",
            "grandeur compound end\n",
            "end compound soundtrack\n",
            "soundtrack compound featur\n",
            "featur compound ladi\n",
            "ladi compound song\n",
            "gaga compound song\n",
            "song compound nowher\n",
            "hold compound nowher\n",
            "hand compound nowher\n",
            "nowher nsubj take\n",
            "near prep nowher\n",
            "icon compound origin\n",
            "chart compound origin\n",
            "topper compound origin\n",
            "origin compound movi\n",
            "movi pobj near\n",
            "take ccomp aim\n",
            "breath dobj take\n",
            "away advmod take\n",
            "tug compound heartstr\n",
            "heartstr dobj take\n",
            "nonetheless advmod action\n",
            "action nsubj sequenc\n",
            "sequenc ccomp take\n",
            "much amod origin\n",
            "thrill compound origin\n",
            "immers compound origin\n",
            "origin nsubj feel\n",
            "feel ccomp aim\n",
            "like prep feel\n",
            "your poss pilot\n",
            "realli compound pilot\n",
            "cockpit compound pilot\n",
            "pilot pobj like\n",
            "that mark flew\n",
            "actor compound intens\n",
            "underw compound intens\n",
            "intens compound train\n",
            "flight compound train\n",
            "train nsubj flew\n",
            "flew ccomp feel\n",
            "actual amod plane\n",
            "plane compound respect\n",
            "shoot compound respect\n",
            "respect ccomp flew\n",
            "top amod maverick\n",
            "gun compound maverick\n",
            "maverick dobj respect\n",
            "feel conj flew\n",
            "like mark took\n",
            "throwback advmod lost\n",
            "lost amod computergener\n",
            "era npadvmod practic\n",
            "practic amod moviemak\n",
            "moviemak compound computergener\n",
            "computergener nmod effect\n",
            "visual amod effect\n",
            "effect nsubj took\n",
            "took advcl feel\n",
            "hollywood compound start\n",
            "start nsubj understand\n",
            "understand ccomp took\n",
            "cruis amod forc\n",
            "creativ compound forc\n",
            "forc dobj understand\n",
            "behind prep understand\n",
            "movi pobj behind\n",
            "driven acl movi\n",
            "make advcl took\n",
            "tell ccomp make\n",
            "stori dobj tell\n",
            "older amod head\n",
            "younger amod head\n",
            "pilot compound butt\n",
            "butt compound head\n",
            "head nsubj stateoftheart\n",
            "stateoftheart ccomp make\n",
            "f compound duke\n",
            "duke dobj stateoftheart\n",
            "rusti compound old\n",
            "old ccomp stateoftheart\n",
            "f dep tri\n",
            "he nsubj tri\n",
            "tri nsubj show\n",
            "show ROOT show\n",
            "u dobj show\n",
            "there advmod show\n",
            "room npadvmod old\n",
            "old amod coexist\n",
            "new amod coexist\n",
            "coexist dobj show\n",
            "he nsubj advanc\n",
            "also advmod advanc\n",
            "advanc ccomp show\n",
            "case compound endur\n",
            "endur amod u\n",
            "appeal compound movi\n",
            "movi compound u\n",
            "power compound transport\n",
            "transport compound u\n",
            "u compound emot\n",
            "viscer compound grip\n",
            "grip nmod action\n",
            "action nmod emot\n",
            "big amod emot\n",
            "sweep compound emot\n",
            "emot dobj advanc\n",
            "Named Entity Recognition: {'CARDINAL': 2, 'PERSON': 2, 'ORG': 3, 'DATE': 1, 'ORDINAL': 1, 'GPE': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constituency Parsing Tree:**Constituency parsing is an analysis of language approach that exposes the sentence's structure of hierarchy by dissecting a sentence into its component components or phrases. Each phrase appears as a node in a semantic parsing tree, and such nodes are arranged in a hierarchical fashion to demonstrate how phrases and words connect to one another inside the sentence. As you descend the node tree, you come across terms like phrases containing nouns (NP), phrases containing verbs (VP), prepositional words (PP), and more. The top-level node often represents the complete sentence. The links between nodes show how various phrases are nested and arranged within the sentence. Each node is identified with the sort of phrase it represents. Recognizing the syntactical framework of helps with constituent parsing.\n",
        "**Dependency Parsing Tree:**Another linguistic analysis method is called dependency parsing, which concentrates on the grammatical links between individual words in an expression rather than on structured phrases. Each word in an expression acts as a node in a relationship's parsing tree, and the borders between nodes stand in for linguistic dependencies or connections. Based on the responsibilities and tasks each word serves in the sentence, the tree shows how they are related to one another. A verb, for instance, may be dependent on its topic, thing, or qualifiers. Prepositions vary according on the terms they refer to. The sort of linguistic connection that every edge in the tree denotes is indicated by a label, such as \"nsubj\" for nominal respondent or \"amod\" for adjective modifier. Recognizing the framework of syntax and grammatical rules through dependency."
      ],
      "metadata": {
        "id": "CUUpatVpooir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeeJIAJJrvKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}